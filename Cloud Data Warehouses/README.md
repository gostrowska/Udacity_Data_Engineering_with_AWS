# Udacity - Data Engeenering with AWS nanodegree - Data Warehouse with AWS Redshift
## Implementing a SQL Data Warehouse using Amazon Redshift

## Requirements
* SQL 
* Python
* [AWS account](https://portal.aws.amazon.com/billing/signup#/start)

## Table of contents
* [Introduction](#Introduction)
* [Project Datasets](#Project-Datasets)
    * [Songs Dataset](#Song-Dataset)
    * [Log Dataset](#Log-Dataset)
* [Star Schema for Song Play Analysis](#Star-Schema-for-Song-Play-Analysis)
* [Project Files](#Project-Files)
* [Project Steps](#Project-Steps)


## Introduction
Sparkify, a music streaming startup, has expanded their user base and song collection, and now they want to migrate their data and processes to the cloud. Currently, their data is stored in Amazon S3, with JSON logs capturing user activity on the app and JSON metadata describing the songs.

This project is to design an ETL pipeline that pulls this data from S3, loads it into Amazon Redshift for staging, and then transforms it into a set of dimensional tables. This setup will enable the analytics team to easily analyze user listening habits and gain valuable insights into their music preferences.

## Project Datasets
There are two datasets stored in S3. Here are the S3 links for each:
- Song data: s3://udacity-dend/song_data
- Log data: s3://udacity-dend/log_data

To properly read log data s3://udacity-dend/log_data, the following metadata file is needed:
- Log metadata: s3://udacity-dend/log_json_path.json

### Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.

>**song_data/A/B/C/TRABCEI128F424C983.json**<br>
>**song_data/A/A/B/TRAABJL12903CDCF1A.json**

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
'''
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
'''

### Log Dataset
The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are file paths to two files in this dataset.

>**log_data/2018/11/2018-11-12-events.json**<br>
>**log_data/2018/11/2018-11-13-events.json**

And below is an example of what the data in a log file, **2018-11-12-events.json**, looks like.

![Log data](./log_data."png)


The log_json_path.json file is used when loading JSON data into Redshift. It specifies the structure of the JSON data so that Redshift can properly parse and load it into the staging tables.

The log_json_path.json file will be used in the COPY command, which is responsible for loading the log data from S3 into the staging tables in Redshift. The log_json_path.json file tells Redshift how to interpret the JSON data and extract the relevant fields. This is essential for further processing and transforming the data into the desired analytics tables.

## Star Schema for Song Play Analysis
**Fact Table:**
- **songplays** - records in event data associated with song plays i.e. records with page NextSong
        songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

**Dimension Tables:**
- **users** - users in the app
        user_id, first_name, last_name, gender, level
- **songs** - songs in music database
        song_id, title, artist_id, year, duration
- **artists** - artists in music database
        artist_id, name, location, latitude, longitude
- **time** - timestamps of records in songplays broken down into specific units
        start_time, hour, day, week, month, year, weekday

![Star_schema]([images/star_schema.png](https://github.com/gostrowska/Udacity_Data_Engineering_with_AWS/blob/main/Cloud%20Data%20Warehouses/star_schema.PNG))


## Project Files
**1. dwh.cfg**                    configuration settings for the database.<br>
**2. sql_queries.py**             SQL statements used in the project, which will be imported into the script files<br>
**3. etl.py**  script file. Loads data from S3 to staging tables on Redshift, then transforms data into a set of dimensional tables <br>
**4. tests.py**            checks the database. <br>
**5. README.md**                  description of a project<br>

## Project Steps
1.  You should have IAM role, IAM user and security group ready. Save KEY and SECRET in dwh.cfg file

        KEY= <YOUR_AWS_KEY>
        SECRET= <YOUR_AWS_SECRET>

        For example:

        KEY=6JW3ATLQ34PH3AKI
        SECRET=wnoBHA+qUBFgwCRHJqgqrLU0i
        
2. Create tables
    Run script **create_tables.py**
    
3. Staging and inserting data (ETL pipeline)
    Run script **etl.py**
   
4. Check database
    Run script **tests.py**
    
5. Close the Redshift cluster

